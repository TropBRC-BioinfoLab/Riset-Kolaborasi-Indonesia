{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy and Pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import time\n",
    "# Iterative Stratification untuk cross validation multilabel\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "#Import Tensorflow dan extension\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Activation\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.callbacks import  EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, Adagrad, SGD, RMSprop, Adadelta\n",
    "import xgboost as xgb\n",
    "#Import keras tuner dan metrics untuk tuning parameter\n",
    "import kerastuner as kt\n",
    "from kerastuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n",
    "from sklearn import metrics\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "#\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifikasi IterativeStratification agar hasil random data tetap sama\n",
    "\n",
    "def new_init(self, n_splits=3, order=1, sample_distribution_per_fold = None, random_state=None):\n",
    "\n",
    "                  self.order = order\n",
    "                  if random_state is not None:\n",
    "                      do_shuffle = True\n",
    "                  else:\n",
    "                      do_shuffle = False\n",
    "                  super(\n",
    "                      IterativeStratification,\n",
    "                      self).__init__(n_splits,\n",
    "                                     shuffle=do_shuffle,\n",
    "                                     random_state=random_state)\n",
    "                  if sample_distribution_per_fold:\n",
    "                      self.percentage_per_fold = sample_distribution_per_fold\n",
    "                  else:\n",
    "                      self.percentage_per_fold = [1 / float(self.n_splits) for _ in range(self.n_splits)]\n",
    "    \n",
    "IterativeStratification.__init__ = new_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAC AAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('E:/temp/feature_2_aac.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pepi=dataset.drop(['pdb_chain','Uniprot_chain','class'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=dataset['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fungsi model SAE\n",
    "def sae_model(xt, xv = None, EPOCHS = 100,BATCH_SIZE = 32, opt = \"adam\",\n",
    "              hl_node = 1024, lr = 0.01,af = \"relu\",num_layers = 3, do=0.5, fr_node = 0.5,\n",
    "              verbose = 0,return_fe = False):\n",
    "  #Setting result placeholders\n",
    "  xt_ae = [] ;xv_ae = [] ; w_ae = []\n",
    "  #If validation set is not present, use train set as validation set\n",
    "  if xv is None :\n",
    "    xv = xt.copy()\n",
    "  opt = tf.keras.optimizers.get(opt) #Set optimizer\n",
    "  K.set_value(opt.learning_rate, lr) #Set learning rate\n",
    "\n",
    "  #Stacked Autoencoder architecture\n",
    "  for n_layers in range(num_layers):\n",
    "    #Autoencoder\n",
    "    inp = Input(shape=(xt.shape[1],))\n",
    "    #Apply Dropout\n",
    "    hidden_layer = Dropout(do)(inp)\n",
    "    #Layer encoder (jumlah layer sesuai dengan n_layers)\n",
    "    enc = Dense(int(hl_node*(fr_node**n_layers)), activation = af)(hidden_layer)  \n",
    "    #Layer Decoder\n",
    "    dec = Dense(xt.shape[1],activation=\"linear\")(enc)\n",
    "    ae = Model(inp, dec)\n",
    "    #Compile model\n",
    "    ae.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    #EarlyStop jika sudah konvergen \n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, verbose=verbose)\n",
    "    #Latih model\n",
    "    ae.fit(xt, xt, \n",
    "           epochs=EPOCHS,batch_size=BATCH_SIZE, \n",
    "           shuffle=True, callbacks = [es] , verbose = verbose,\n",
    "           validation_data = (xv,xv))\n",
    "    #Ekstrak Feature extraction\n",
    "    fe = Model(ae.input, enc)\n",
    "    #Simpan data hasil latih\n",
    "    xt = fe.predict(xt) ; xt_ae.append(xt)\n",
    "    xv = fe.predict(xv) ; xv_ae.append(xv)\n",
    "    #Simpan bobot hasil latih SAE\n",
    "    w_ae.append([layer_name for layer_name in ae.layers if \"dense\" in layer_name.name][0].get_weights())\n",
    "    if verbose:\n",
    "      print(\"Layer {} trained\".format(n_layers+1))\n",
    "\n",
    "  return (w_ae,xv) if return_fe else w_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fungsi DNN\n",
    "def dnn_model(xt, n_outputs = 1, sae_weights = None, EPOCHS = 100,BATCH_SIZE = 32, opt = \"adam\",\n",
    "              hl_node = 1024, lr = 0.01,af = \"relu\",num_layers = 3, do=0.5, fr_node = 0.5):\n",
    "  opt = tf.keras.optimizers.get(opt) #Set optimizer\n",
    "  K.set_value(opt.learning_rate, lr) #Set learning rate\n",
    "  \n",
    "  #Model architecture\n",
    "  input_layer = Input(shape=(xt.shape[1],))\n",
    "  hidden_layer = BatchNormalization()(input_layer)\n",
    "  hidden_layer = Dropout(do)(hidden_layer)\n",
    "#Set jumlah hidden layer\n",
    "  for n_layers in range(num_layers):\n",
    "    hidden_layer = Dense(int(hl_node*(fr_node**n_layers)), activation = af)(hidden_layer)\n",
    "    hidden_layer = BatchNormalization()(hidden_layer)\n",
    "    hidden_layer = Dropout(do)(hidden_layer)\n",
    "  output_layer = Dense(n_outputs, activation = 'sigmoid')(hidden_layer)\n",
    "#latih model\n",
    "  dnn = Model(input_layer, output_layer)\n",
    "\n",
    "  #Latih model DNN dengan bobot SAE (jika bobot ada)\n",
    "  if sae_weights is not None:\n",
    "    weights = sae_weights\n",
    "    dnn_dense = [layer_name for layer_name in dnn.layers if \"dense\" in layer_name.name]\n",
    "    for weight_from,weight_to in list(zip(weights,dnn_dense)):\n",
    "      weight_to.set_weights(weight_from)\n",
    "\n",
    "  #Compile model\n",
    "  dnn.compile(optimizer=opt, loss='binary_crossentropy', metrics = [\n",
    "               tf.keras.metrics.Precision(),\n",
    "               tf.keras.metrics.Recall()],\n",
    "               )\n",
    "  return dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding sae weights....\n",
      "done, processing time: 0.0\n",
      "Average Result of 5 CV\n",
      "Accuracy    : 0.81132±0.002\n",
      "Recall      : 0.04578±0.013\n",
      "Precision   : 0.81537±0.031\n",
      "ROC-AUC     : 0.52163±0.006\n",
      "F1 Score    : 0.08636±0.023\n",
      "[[6970   19]\n",
      " [1601   98]]\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "#Train SAE-DNN\n",
    "#Variabel untuk simpan hasil\n",
    "res_all = [[],[],[],[],[]]\n",
    "auc_plots = []\n",
    "#Latih SAE \n",
    "print(\"finding sae weights....\")\n",
    "ti0 = time.time()\n",
    "# sae_weights = sae_model(xt = X_pepi)\n",
    "ti1 = time.time()\n",
    "print('done, processing time:', ti1-ti0)\n",
    "i=0\n",
    "t0 = time.time()\n",
    "#Inisialisasi CV\n",
    "np.random.seed(123)\n",
    "cv = StratifiedKFold(n_splits=5, random_state=42,shuffle=True)\n",
    "#Mulai latih DNN dengan bobot hasil SAE untuk tiap CV\n",
    "for train_ix, test_ix in cv.split(X_pepi,Y):\n",
    "    #Bagi data menjadi train, test\n",
    "    X_train, X_test = X_pepi.iloc[train_ix,:], X_pepi.iloc[test_ix,:]\n",
    "    y_train, y_test = Y[train_ix], Y[test_ix]  \n",
    "    # define model dengan bobot SAE. Jika tidak memakai bobot SAE, sae_weights = None\n",
    "    model = dnn_model(xt = X_train, sae_weights = sae_weights)\n",
    "    # latih model\n",
    "    model.fit(X_train, y_train, verbose=False, epochs=100)\n",
    "    # Prediksi test \n",
    "    yhat = model.predict(X_test)\n",
    "    # Bulatkan hasil prediksi (probabilitas)\n",
    "    yhat = yhat.round()\n",
    "    # Hitung metrik\n",
    "    #Calculate metrics\n",
    "    accu = accuracy_score(y_test, yhat)\n",
    "    auc = roc_auc_score(y_test, yhat)\n",
    "    precision_score,recall_score, f1_score,_ = precision_recall_fscore_support(y_test, yhat, average='binary',pos_label=1)\n",
    "    _,speci,_,_ = precision_recall_fscore_support(y_test, yhat, average='binary',pos_label=0)\n",
    "    \n",
    "    res_all[0].append(accu);res_all[1].append(recall_score);res_all[2].append(precision_score);res_all[3].append(auc);res_all[4].append(f1_score)\n",
    "    fpr, tpr, _ = roc_curve(y_test,  yhat)\n",
    "    auc_plots.append([fpr,tpr,auc])\n",
    " \n",
    "#Average and Stdv of k-fold CV\n",
    "print('Average Result of {} CV'.format(5))\n",
    "print('Accuracy    : {0:.5f}±{1:.3f}'.format(np.mean(res_all[0]), np.std(res_all[0])))\n",
    "print('Recall      : {0:.5f}±{1:.3f}'.format(np.mean(res_all[1]), np.std(res_all[1])))\n",
    "print('Precision   : {0:.5f}±{1:.3f}'.format(np.mean(res_all[2]), np.std(res_all[2])))\n",
    "print('ROC-AUC     : {0:.5f}±{1:.3f}'.format(np.mean(res_all[3]), np.std(res_all[3])))\n",
    "print('F1 Score    : {0:.5f}±{1:.3f}'.format(np.mean(res_all[4]), np.std(res_all[4])))\n",
    "print(confusion_matrix(y_test,yhat))\n",
    "print('===================================')\n",
    "\n",
    "#Choose auc plot with highest score\n",
    "best_auc_pubchem_aac = auc_plots[np.array(res_all[3]).argmax()]\n",
    "res_all_pubchem_aac = res_all\n",
    "\n",
    "#save model\n",
    "model.save(\"SAE-DNN PubChem Biner pepi.h5\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
